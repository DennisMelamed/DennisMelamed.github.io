
@inproceedings{sun_idol_2021,
	title = {{IDOL}: {Inertial} {Deep} {Orientation}-{Estimation} and {Localization}},
	volume = {35},
	shorttitle = {{IDOL}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/16763},
	doi = {10.1609/aaai.v35i7.16763},
	abstract = {Many smartphone applications use inertial measurement units (IMUs) to sense movement, but the use of these sensors for pedestrian localization can be challenging due to their noise characteristics. Recent data-driven inertial odometry approaches have demonstrated the increasing feasibility of inertial navigation. However, they still rely upon conventional smartphone orientation estimates that they assume to be accurate, while in fact these orientation estimates can be a significant source of error. To address the problem of inaccurate orientation estimates, we present a two-stage, data-driven pipeline using a commodity smartphone that first estimates device orientations and then estimates device position. The orientation module relies on a recurrent neural network and Extended Kalman Filter to obtain orientation estimates that are used to then rotate raw IMU measurements into the appropriate reference frame. The position module then passes those measurements through another recurrent network architecture to perform localization. Our proposed method outperforms state-of-the-art methods in both orientation and position error on a large dataset we constructed that contains 20 hours of pedestrian motion across 3 buildings and 15 subjects. Code and data are available at https://github.com/KlabCMU/IDOL.},
	urldate = {2024-10-19},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Sun, Scott and Melamed, Dennis and Kitani, Kris},
	month = may,
	year = {2021},
	pages = {6128--6137},
	file = {Full Text:/Users/dennismelamed/Zotero/storage/232KL3IF/Sun et al. - 2021 - IDOL Inertial Deep Orientation-Estimation and Localization.pdf:application/pdf},
}

@inproceedings{melamed_learnable_2022,
	title = {Learnable {Spatio}-{Temporal} {Map} {Embeddings} for {Deep} {Inertial} {Localization}},
	url = {https://ieeexplore.ieee.org/document/9981092},
	doi = {10.1109/IROS47612.2022.9981092},
	abstract = {Indoor localization systems often fuse inertial odometry with map information via hand-defined methods to reduce odometry drift, but such methods are sensitive to noise and struggle to generalize across odometry sources. To address the robustness problem in map utilization, we propose a data-driven prior on possible user locations in a map by combining learned spatial map embeddings and temporal odometry embeddings. Our prior learns to encode which map regions are feasible locations for a user more accurately than previous hand-defined methods. This prior leads to a 49\% improvement in inertial-only localization accuracy when used in a particle filter. This result is significant, as it shows that our relative positioning method can match the performance of absolute positioning using bluetooth beacons. To show the gen-eralizability of our method, we also show similar improvements using wheel encoder odometry. Our code will be made publicly availableâ€ 1project page: https://rebrand.ly/learned-map-prior.},
	urldate = {2024-10-19},
	booktitle = {2022 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Melamed, Dennis and Ram, Karnik and Roy, Vivek and Kitani, Kris},
	month = oct,
	year = {2022},
	note = {ISSN: 2153-0866},
	keywords = {Fuses, Location awareness, Particle filters, Performance evaluation, Robot sensing systems, Robustness, Wheels},
	pages = {6984--6990},
	file = {IEEE Xplore Abstract Record:/Users/dennismelamed/Zotero/storage/AIBFQSB7/9981092.html:text/html;Submitted Version:/Users/dennismelamed/Zotero/storage/2NDX6B4T/Melamed et al. - 2022 - Learnable Spatio-Temporal Map Embeddings for Deep Inertial Localization.pdf:application/pdf},
}

@inproceedings{melamed_uncovering_2024,
	title = {Uncovering {Bias} in {Building} {Damage} {Assessment} from {Satellite} {Imagery}},
	url = {https://ieeexplore.ieee.org/document/10642347},
	doi = {10.1109/IGARSS53475.2024.10642347},
	abstract = {We identify a bias in a commonly used dataset for building damage detection, evaluate its effects on existing deep learning models, and devise mitigation strategies to overcome it. We find that the data contains significantly more groups of damaged buildings than single ones leading to skewed machine learning evaluations. Consequently, deep learning models heavily rely on surrounding context rather than individual building damage when classifying supporting our claim. Specifically, the dataset includes extraneous damage surrounding buildings such as debris, fallen trees, and other damaged buildings which results in deep neural networks overfitting to these features. We analyze the top-5 solutions of the xView2 challenge, which focuses on building damage classification using satellite imagery as provided by the xBD dataset. Our experiments reveal that these models struggle to accurately identify isolated damaged buildings, potentially causing oversights in critical disaster scenarios and delaying humanitarian aid. Finally, we devise a new augmentation strategy to reduce this bias in disaster datasets and show it improves real-world outcomes.},
	urldate = {2024-10-19},
	booktitle = {{IGARSS} 2024 - 2024 {IEEE} {International} {Geoscience} and {Remote} {Sensing} {Symposium}},
	author = {Melamed, Dennis and Johnson, Cameron and Gerg, Isaac D. and Zhao, Chen and Blue, Russell and Hoogs, Anthony and Clipp, Brian and Morrone, Philip},
	month = jul,
	year = {2024},
	note = {ISSN: 2153-7003},
	keywords = {Building Damage Detection, Buildings, Dataset Bias, Deep learning, Deep Learning, Disasters, Humanitarian activities, Machine Learning, Measurement, Prevention and mitigation, Training, xBD, xView2},
	pages = {8095--8099},
	file = {IEEE Xplore Abstract Record:/Users/dennismelamed/Zotero/storage/G7YT94E7/10642347.html:text/html},
}

@inproceedings{davila_multi-atr_2023,
	title = {Multi-{ATR} {Fusion} and {Ontological} {Deconfliction} for {Geospatial} {Imagery}},
	language = {en},
	booktitle = {Proceedings of the {National} {Security} {Sensor} and {Data} {Fusion} {Committee} ({NSSDF})},
	author = {Davila, D. and Melamed, D. and Depauw, D. and Anderson, J.},
	year = {2023},
}

@inproceedings{melamed_rapid_2023,
	title = {Rapid {Training} of {Artificial} {Intelligence} {Battle} {Damage} {Assessment} {Tools} to {New} {Conflicts}},
	language = {en},
	booktitle = {Proceedings of the {National} {Security} {Sensor} and {Data} {Fusion} {Committee} ({NSSDF})},
	author = {Melamed, D. and Johnson, C. and Brockman, S. and Blue, R. and Hoogs, A. and Morrone, P. and Clipp, B.},
	year = {2023},
}

@inproceedings{reed_ebs-ekf_2025,
	title = {{EBS}-{EKF}: {Accurate} and {High} {Frequency} {Event}-based {Star} {Tracking}},
	shorttitle = {{EBS}-{EKF}},
	url = {http://arxiv.org/abs/2503.20101},
	doi = {10.48550/arXiv.2503.20101},
	abstract = {Event-based sensors (EBS) are a promising new technology for star tracking due to their low latency and power efficiency, but prior work has thus far been evaluated exclusively in simulation with simplified signal models. We propose a novel algorithm for event-based star tracking, grounded in an analysis of the EBS circuit and an extended Kalman filter (EKF). We quantitatively evaluate our method using real night sky data, comparing its results with those from a space-ready active-pixel sensor (APS) star tracker. We demonstrate that our method is an order-of-magnitude more accurate than existing methods due to improved signal modeling and state estimation, while providing more frequent updates and greater motion tolerance than conventional APS trackers. We provide all code and the first dataset of events synchronized with APS solutions.},
	urldate = {2025-04-02},
	booktitle = {2025 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {arXiv},
	author = {Reed, Albert W. and Hashemi, Connor and Melamed, Dennis and Menon, Nitesh and Hirakawa, Keigo and McCloskey, Scott},
	month = jun,
	year = {2025},
	note = {arXiv:2503.20101 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/Users/dennismelamed/Zotero/storage/QIB59G9A/Reed et al. - 2025 - EBS-EKF Accurate and High Frequency Event-based Star Tracking.pdf:application/pdf;Snapshot:/Users/dennismelamed/Zotero/storage/5FPD8SS4/2503.html:text/html},
}
