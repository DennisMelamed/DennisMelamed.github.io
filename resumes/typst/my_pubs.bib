
@inproceedings{reed_ebs-ekf_2025,
	title = {{EBS}-{EKF}: {Accurate} and {High} {Frequency} {Event}-based {Star} {Tracking}},
	shorttitle = {{EBS}-{EKF}},
	url = {https://openaccess.thecvf.com/content/CVPR2025/html/Reed_EBS-EKF_Accurate_and_High_Frequency_Event-based_Star_Tracking_CVPR_2025_paper.html},
	language = {en},
	urldate = {2025-07-03},
	booktitle = {Proceedings of the 2025 {Computer} {Vision} and {Pattern} {Recognition} ({CVPR}) {Conference}},
	author = {Reed, Albert W. and Hashemi, Connor and Melamed, Dennis and Menon, Nitesh and Hirakawa, Keigo and McCloskey, Scott},
	year = {2025},
	pages = {6510--6519},
}

@inproceedings{melamed_quantifying_2025,
	address = {Honolulu, Hawaii, United States},
	title = {Quantifying {Accuracy} of an {Event}-{Based} {Star} {Tracker} via {Earth}’s {Rotation}},
	url = {https://openaccess.thecvf.com/content/ICCV2025W/NeVi/html/Melamed_Quantifying_Accuracy_of_an_Event-Based_Star_Tracker_via_Earths_Rotation_ICCVW_2025_paper.html},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV}) {Workshops}},
	author = {Melamed, Dennis and Hashemi, Connor and McCloskey, Scott},
	month = oct,
	year = {2025},
	pages = {4637--4644},
}

@inproceedings{sun_idol_2021,
	title = {{IDOL}: {Inertial} {Deep} {Orientation}-{Estimation} and {Localization}},
	volume = {35},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	shorttitle = {{IDOL}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/16763},
	doi = {10.1609/aaai.v35i7.16763},
	abstract = {Many smartphone applications use inertial measurement units (IMUs) to sense movement, but the use of these sensors for pedestrian localization can be challenging due to their noise characteristics. Recent data-driven inertial odometry approaches have demonstrated the increasing feasibility of inertial navigation. However, they still rely upon conventional smartphone orientation estimates that they assume to be accurate, while in fact these orientation estimates can be a significant source of error. To address the problem of inaccurate orientation estimates, we present a two-stage, data-driven pipeline using a commodity smartphone that first estimates device orientations and then estimates device position. The orientation module relies on a recurrent neural network and Extended Kalman Filter to obtain orientation estimates that are used to then rotate raw IMU measurements into the appropriate reference frame. The position module then passes those measurements through another recurrent network architecture to perform localization. Our proposed method outperforms state-of-the-art methods in both orientation and position error on a large dataset we constructed that contains 20 hours of pedestrian motion across 3 buildings and 15 subjects. Code and data are available at https://github.com/KlabCMU/IDOL.},
	language = {en},
	urldate = {2025-07-03},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Sun, Scott and Melamed, Dennis and Kitani, Kris},
	month = may,
	year = {2021},
	note = {Number: 7},
	keywords = {Applications},
	pages = {6128--6137},
}

@inproceedings{hashemi_centroiding_2025,
	address = {Toronto, Ontario, Canada},
	title = {Centroiding {Point}-{Objects} with {Event} {Cameras}},
	booktitle = {Proceedings of the 2025 {IEEE} {International} {Conference} on {Computation} {Photography} ({ICCP})},
	author = {Hashemi, Connor and Melamed, Dennis and McCloskey, Scott},
	month = jul,
	year = {2025},
}

@inproceedings{melamed_rapid_2023,
	title = {Rapid {Training} of {Artificial} {Intelligence} {Battle} {Damage} {Assessment} {Tools} to {New} {Conflicts}},
	language = {en},
	booktitle = {Proceedings of the {National} {Security} {Sensor} and {Data} {Fusion} {Committee} ({NSSDF})},
	author = {Melamed, D. and Johnson, C. and Brockman, S. and Blue, R. and Hoogs, A. and Morrone, P. and Clipp, B.},
	year = {2023},
}

@inproceedings{davila_multi-atr_2023,
	title = {Multi-{ATR} {Fusion} and {Ontological} {Deconfliction} for {Geospatial} {Imagery}},
	language = {en},
	booktitle = {Proceedings of the {National} {Security} {Sensor} and {Data} {Fusion} {Committee} ({NSSDF})},
	author = {Davila, D. and Melamed, D. and Depauw, D. and Anderson, J.},
	year = {2023},
}

@inproceedings{melamed_uncovering_2024,
	title = {Uncovering {Bias} in {Building} {Damage} {Assessment} from {Satellite} {Imagery}},
	url = {https://ieeexplore.ieee.org/document/10642347},
	doi = {10.1109/IGARSS53475.2024.10642347},
	abstract = {We identify a bias in a commonly used dataset for building damage detection, evaluate its effects on existing deep learning models, and devise mitigation strategies to overcome it. We find that the data contains significantly more groups of damaged buildings than single ones leading to skewed machine learning evaluations. Consequently, deep learning models heavily rely on surrounding context rather than individual building damage when classifying supporting our claim. Specifically, the dataset includes extraneous damage surrounding buildings such as debris, fallen trees, and other damaged buildings which results in deep neural networks overfitting to these features. We analyze the top-5 solutions of the xView2 challenge, which focuses on building damage classification using satellite imagery as provided by the xBD dataset. Our experiments reveal that these models struggle to accurately identify isolated damaged buildings, potentially causing oversights in critical disaster scenarios and delaying humanitarian aid. Finally, we devise a new augmentation strategy to reduce this bias in disaster datasets and show it improves real-world outcomes.},
	urldate = {2025-07-03},
	booktitle = {{IGARSS} 2024 - 2024 {IEEE} {International} {Geoscience} and {Remote} {Sensing} {Symposium}},
	author = {Melamed, Dennis and Johnson, Cameron and Gerg, Isaac D. and Zhao, Chen and Blue, Russell and Hoogs, Anthony and Clipp, Brian and Morrone, Philip},
	month = jul,
	year = {2024},
	note = {ISSN: 2153-7003},
	keywords = {Building Damage Detection, Buildings, Dataset Bias, Deep Learning, Deep learning, Disasters, Humanitarian activities, Machine Learning, Measurement, Prevention and mitigation, Training, xBD, xView2},
	pages = {8095--8099},
}

@inproceedings{melamed_learnable_2022,
	title = {Learnable {Spatio}-{Temporal} {Map} {Embeddings} for {Deep} {Inertial} {Localization}},
	url = {https://ieeexplore.ieee.org/document/9981092},
	doi = {10.1109/IROS47612.2022.9981092},
	abstract = {Indoor localization systems often fuse inertial odometry with map information via hand-defined methods to reduce odometry drift, but such methods are sensitive to noise and struggle to generalize across odometry sources. To address the robustness problem in map utilization, we propose a data-driven prior on possible user locations in a map by combining learned spatial map embeddings and temporal odometry embeddings. Our prior learns to encode which map regions are feasible locations for a user more accurately than previous hand-defined methods. This prior leads to a 49\% improvement in inertial-only localization accuracy when used in a particle filter. This result is significant, as it shows that our relative positioning method can match the performance of absolute positioning using bluetooth beacons. To show the gen-eralizability of our method, we also show similar improvements using wheel encoder odometry. Our code will be made publicly available†1project page: https://rebrand.ly/learned-map-prior.},
	urldate = {2025-07-03},
	booktitle = {2022 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Melamed, Dennis and Ram, Karnik and Roy, Vivek and Kitani, Kris},
	month = oct,
	year = {2022},
	note = {ISSN: 2153-0866},
	keywords = {Fuses, Location awareness, Particle filters, Performance evaluation, Robot sensing systems, Robustness, Wheels},
	pages = {6984--6990},
}
